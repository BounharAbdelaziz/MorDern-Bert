# -*- coding: utf-8 -*-
"""pretrain_modern_bert_darija_tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z2ojWGa7b3ZAv17WfSxIiFr4AiWgxicT
"""

from moder_bert_utils import *

load_dotenv()
HF_READ_KEY=os.environ["HF_READ_KEY"]
DATASET_NAME=os.environ["DATASET_NAME"]

if __name__ == "__main__":
    info("load dataset")
    dataset=load_dataset(DATASET_NAME,token=HF_READ_KEY)
    info("dataset loaded!")

    configs=Config()
    info("load base tokenizer")
    base_tokenizer=AutoTokenizer.from_pretrained(
        "answerdotai/ModernBERT-base",
        use_fast=True # Fast tokenizers are implemented in Rust and are significantly faster than the regular Python-based tokenizers.
    )

    info("train new Darija tokenizer")
    train_iterator=batch_iter(dataset["train"])
    new_tokenizer=base_tokenizer.train_new_from_iterator(
        text_iterator=train_iterator,
        vocab_size=configs.new_vocab_size,
        show_progress=True
    )

    info("Save new tokenizer...")
    new_tokenizer.save_pretrained(f"{configs.base_dir}/tokenizer")
    info("new tokenizer saved...")

    info("load new Darija tokenizer")
    new_tokenizer=AutoTokenizer.from_pretrained(f"{configs.base_dir}/tokenizer",use_fast=True)

    info("tokenize train/test dataset...")
    train_dataset=dataset["train"].map(
        lambda example: process(example,new_tokenizer,configs),
        batched=True,
        remove_columns=dataset["train"].column_names
    )
    test_dataset=dataset["test"].map(
        lambda example: process(example,new_tokenizer,configs),
        batched=True,
        remove_columns=dataset["test"].column_names
    )
    info("Done!")

    info("init data collator...")
    data_collator=DataCollatorForLanguageModeling(
        tokenizer=new_tokenizer,
        mlm=True,
        mlm_probability=configs.mlm_probability
    )

    info("load model...")
    model=AutoModelForMaskedLM.pretrained(
        configs.model_name
    )
    info("Done!")

    info("resize embedding matrix...")
    model.resize_token_embeddings(configs.new_tokenizer_voca_size)
    info("Done!")

    info("init training args...")
    training_args = TrainingArguments(
        output_dir=configs.output_dir,
        overwrite_output_dir=configs.overwrite_output_dir,
        num_train_epochs=configs.num_train_epochs,
        per_device_train_batch_size=configs.per_device_train_batch_size,
        per_device_eval_batch_size=configs.per_device_eval_batch_size,
        evaluation_strategy=configs.evaluation_strategy,
        eval_steps=configs.eval_steps,
        logging_steps=configs.logging_steps,
        save_steps=configs.save_steps,
        save_total_limit=configs.save_total_limit,
        learning_rate=configs.learning_rate,
        warmup_steps=configs.warmup_steps,
        weight_decay=configs.weight_decay,
        report_to=configs.report_to,
        run_name=configs.run_name,
    )
    info("Done!")

    info("init trainer...")
    trainer=Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        data_collator=data_collator
    )
    info("Done!")

    info("trainer...")
    trainer.train()

    info("save result model...")
    trainer.save_model(configs.output_dir)
    new_tokenizer.save_pretrained(configs.output_dir)
    info("push result model to hub...")
    trainer.push_to_hub("atlasia/modern-bert-darija")
    info("Done!")
